{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff28d06",
   "metadata": {},
   "source": [
    "# Live Web Scraping & Word Cloud Generator\n",
    "\n",
    "## ðŸ“‹ Task Overview\n",
    "This notebook demonstrates **real-time web scraping** and **text visualization** using Python. The goal is to:\n",
    "\n",
    "1. **Extract headlines/text** from live websites\n",
    "2. **Clean and process** the scraped content\n",
    "3. **Generate visual word clouds** to identify trending topics and themes\n",
    "4. **Handle errors gracefully** for robust scraping\n",
    "\n",
    "## ðŸ”§ Solution Architecture\n",
    "\n",
    "### Core Components:\n",
    "- **`simple_scrape_headlines()`** - Flexible web scraper with error handling\n",
    "- **`clean_text_list()`** - Text preprocessing and stopword filtering  \n",
    "- **`make_wordcloud()`** - Visual word cloud generation\n",
    "\n",
    "### Key Features:\n",
    "- âœ… **Robust Error Handling** - Handles timeouts, connection errors, HTTP errors\n",
    "- âœ… **Flexible Targeting** - Supports custom CSS selectors or automatic tag detection\n",
    "- âœ… **Smart Text Cleaning** - Removes URLs, special characters, and stopwords\n",
    "- âœ… **Visual Feedback** - Progress updates and extraction statistics\n",
    "- âœ… **Configurable Parameters** - Timeout, max items, custom stopwords\n",
    "\n",
    "### Technologies Used:\n",
    "- **`requests`** - HTTP requests and web scraping\n",
    "- **`BeautifulSoup`** - HTML parsing and element extraction\n",
    "- **`WordCloud`** - Word cloud visualization\n",
    "- **`matplotlib`** - Plotting and display\n",
    "- **`nltk`** - Natural language processing and stopwords\n",
    "- **`re`** - Regular expressions for text cleaning\n",
    "\n",
    "## ðŸŽ¯ Use Cases\n",
    "- **News Trend Analysis** - Monitor trending topics across news sites\n",
    "- **Content Research** - Analyze themes from multiple sources\n",
    "- **Social Media Insights** - Extract popular discussions\n",
    "- **Market Research** - Track industry conversations\n",
    "- **Educational Analysis** - Study content patterns\n",
    "\n",
    "## ðŸš€ Quick Start\n",
    "1. Run the installation cell to install required packages\n",
    "2. Execute the main scraping cell to see TechCrunch analysis\n",
    "3. Try different websites using the example cells below\n",
    "4. Customize parameters for your specific needs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6a293",
   "metadata": {},
   "source": [
    "https://news.ycombinator.com/\n",
    " (Hacker News front page â€” headlines)\n",
    "\n",
    "https://www.bbc.com/news\n",
    " (BBC News front page headlines) â€” prefer /rss or APIs if heavy scraping\n",
    "\n",
    "https://en.wikipedia.org/wiki/Machine_learning\n",
    " (section titles & lead paragraph)\n",
    "\n",
    "https://techcrunch.com/\n",
    " (tech headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c4fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests beautifulsoup4 wordcloud matplotlib nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0f6617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Live Web Scraping and Word Cloud Generation ===\n",
      "\n",
      "Fetching content from: https://www.reuters.com/technology/\n",
      "Error: HTTP error occurred: 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/technology/\n",
      "No headlines were extracted. Please check the URL or your internet connection.\n",
      "Error: HTTP error occurred: 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/technology/\n",
      "No headlines were extracted. Please check the URL or your internet connection.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "# Download NLTK stopwords (run once)\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    from nltk.corpus import stopwords\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not download NLTK stopwords: {e}\")\n",
    "    print(\"Using basic English stopwords instead\")\n",
    "    stopwords = None\n",
    "\n",
    "def simple_scrape_headlines(url, selector=None, max_items=50, timeout=10):\n",
    "    \"\"\"\n",
    "    Simple headline scraper:\n",
    "    - url: page to fetch\n",
    "    - selector: optional CSS selector for headlines (if None we try common tags)\n",
    "    - max_items: limit number of extracted items\n",
    "    - timeout: request timeout in seconds\n",
    "    Returns: list of strings (titles / snippets)\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (compatible; DemoBot/1.0)'}\n",
    "    \n",
    "    try:\n",
    "        print(f\"Fetching content from: {url}\")\n",
    "        resp = requests.get(url, headers=headers, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        print(f\"Successfully fetched page (status: {resp.status_code})\")\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Error: Request timed out after {timeout} seconds\")\n",
    "        return []\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Error: Could not connect to the website. Check your internet connection.\")\n",
    "        return []\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"Error: HTTP error occurred: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Unexpected error occurred while fetching: {e}\")\n",
    "        return []\n",
    "\n",
    "    texts = []\n",
    "    if selector:\n",
    "        try:\n",
    "            elems = soup.select(selector)\n",
    "            for el in elems[:max_items]:\n",
    "                t = el.get_text(strip=True)\n",
    "                if t:\n",
    "                    texts.append(t)\n",
    "        except Exception as e:\n",
    "            print(f\"Error using custom selector '{selector}': {e}\")\n",
    "            return []\n",
    "    else:\n",
    "        # try common headline tags\n",
    "        for tag in ['h1','h2','h3','a','title']:\n",
    "            try:\n",
    "                elems = soup.find_all(tag)\n",
    "                for el in elems:\n",
    "                    t = el.get_text(strip=True)\n",
    "                    if t and len(t.split()) < 40:  # avoid huge text blocks\n",
    "                        texts.append(t)\n",
    "                    if len(texts) >= max_items:\n",
    "                        break\n",
    "                if len(texts) >= max_items:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing {tag} tags: {e}\")\n",
    "                continue\n",
    "\n",
    "    # dedupe and return\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for t in texts:\n",
    "        if t not in seen and len(t.strip()) > 2:  # filter very short texts\n",
    "            seen.add(t)\n",
    "            out.append(t)\n",
    "    \n",
    "    print(f\"Extracted {len(out)} unique headlines/texts\")\n",
    "    return out[:max_items]\n",
    "\n",
    "def clean_text_list(texts, extra_stopwords=None):\n",
    "    \"\"\"Simple cleaning: lowercase, remove punctuation, filter stopwords\"\"\"\n",
    "    if not texts:\n",
    "        print(\"Warning: No texts to clean\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Use NLTK stopwords if available, otherwise use basic list\n",
    "    if stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    else:\n",
    "        stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
    "    \n",
    "    if extra_stopwords:\n",
    "        stop_words |= set([w.lower() for w in extra_stopwords])\n",
    "    \n",
    "    joined = \" \".join(texts).lower()\n",
    "    \n",
    "    # remove URLs, special chars, digits\n",
    "    joined = re.sub(r'http\\S+',' ', joined)\n",
    "    joined = re.sub(r'[^a-z\\s]', ' ', joined)\n",
    "    \n",
    "    # tokenize and filter\n",
    "    tokens = [w for w in joined.split() if len(w) > 2 and w not in stop_words]\n",
    "    \n",
    "    print(f\"Cleaned text contains {len(tokens)} words\")\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def make_wordcloud(text, title=None, max_words=120):\n",
    "    \"\"\"Generate and display word cloud\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        print(\"Error: No text provided for word cloud generation\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        wc = WordCloud(width=900, height=400, background_color='white',\n",
    "                       max_words=max_words, stopwords=set(STOPWORDS),\n",
    "                       colormap='viridis')  # Added colormap for better visuals\n",
    "        wc.generate(text)\n",
    "        \n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        if title:\n",
    "            plt.title(title, fontsize=16, pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating word cloud: {e}\")\n",
    "\n",
    "# Example usage with improved error handling:\n",
    "print(\"=== Live Web Scraping and Word Cloud Generation ===\")\n",
    "print()\n",
    "\n",
    "url = \"https://www.reuters.com/technology/\"  # replace with chosen URL\n",
    "headlines = simple_scrape_headlines(url, selector=None, max_items=80)\n",
    "\n",
    "if headlines:\n",
    "    print(f\"\\nSample items (first {min(10, len(headlines))}):\")\n",
    "    for i, item in enumerate(headlines[:10], 1):\n",
    "        print(f\"{i:2}. {item}\")\n",
    "    \n",
    "    clean = clean_text_list(headlines, extra_stopwords=['news','read','said','hacker','hn'])\n",
    "    if clean:\n",
    "        make_wordcloud(clean, title=f\"WordCloud from {url}\")\n",
    "    else:\n",
    "        print(\"No clean text available for word cloud generation\")\n",
    "else:\n",
    "    print(\"No headlines were extracted. Please check the URL or your internet connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4085669e",
   "metadata": {},
   "source": [
    "## Try Different Websites\n",
    "\n",
    "You can now easily try scraping different websites. Here are some examples:\n",
    "\n",
    "### Popular News Sites:\n",
    "- **BBC News**: https://www.bbc.com/news\n",
    "- **CNN**: https://www.cnn.com  \n",
    "- **Reddit**: https://www.reddit.com\n",
    "- **TechCrunch**: https://techcrunch.com\n",
    "\n",
    "### Usage Examples:\n",
    "```python\n",
    "# Try a different site\n",
    "url = \"https://www.bbc.com/news\"\n",
    "headlines = simple_scrape_headlines(url, max_items=50)\n",
    "clean_text = clean_text_list(headlines, extra_stopwords=['bbc', 'news', 'uk'])\n",
    "make_wordcloud(clean_text, title=f\"WordCloud from BBC News\")\n",
    "```\n",
    "\n",
    "### Custom CSS Selectors:\n",
    "For more targeted scraping, you can use CSS selectors:\n",
    "```python\n",
    "# Example with CSS selector for specific elements\n",
    "headlines = simple_scrape_headlines(\n",
    "    \"https://news.ycombinator.com/\", \n",
    "    selector=\"a.storylink\",  # Specific HN headline links\n",
    "    max_items=30\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31b4a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to try different websites! Uncomment the examples above or add your own URL.\n"
     ]
    }
   ],
   "source": [
    "# Example: Try scraping a different website\n",
    "# Uncomment and modify the lines below to try different sites:\n",
    "\n",
    "# Option 1: Reddit front page\n",
    "# url = \"https://www.reddit.com\"\n",
    "# headlines = simple_scrape_headlines(url, max_items=60)\n",
    "# clean_text = clean_text_list(headlines, extra_stopwords=['reddit', 'comments', 'vote', 'share'])\n",
    "# make_wordcloud(clean_text, title=\"WordCloud from Reddit\")\n",
    "\n",
    "# Option 2: BBC News  \n",
    "# url = \"https://www.bbc.com/news\"\n",
    "# headlines = simple_scrape_headlines(url, max_items=50)\n",
    "# clean_text = clean_text_list(headlines, extra_stopwords=['bbc', 'news', 'uk', 'more'])\n",
    "# make_wordcloud(clean_text, title=\"WordCloud from BBC News\")\n",
    "\n",
    "# Option 3: Custom website of your choice\n",
    "# url = \"YOUR_URL_HERE\"\n",
    "# headlines = simple_scrape_headlines(url, max_items=50)\n",
    "# clean_text = clean_text_list(headlines)\n",
    "# make_wordcloud(clean_text, title=f\"WordCloud from {url}\")\n",
    "\n",
    "print(\"Ready to try different websites! Uncomment the examples above or add your own URL.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
